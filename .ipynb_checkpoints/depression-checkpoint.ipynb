{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ftfy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from math import exp\n",
    "from numpy import sign\n",
    "\n",
    "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D, concatenate \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed() is used to generate same set of numbers before rand() function is called\n",
    "#random numbers work by starting with a number (the seed), multiplying it by a large number, \n",
    "#then taking modulo of that product. The resulting number is then used as the seed to generate the next \"random\" number.\n",
    "#When you set the seed (every time), it does the same thing every time, giving you the same numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Both the depressive tweets and the random tweets are scrapped from twitter using twint and in addition \"depression\" keyword is used while collecting depressive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_df=pd.read_csv('depressive_tweets.csv',sep=',',header=None,usecols=range(0,10),nrows=3200)\n",
    "#file should be in same path as ipynb file\n",
    "#sep is by which fields are seperated.\n",
    "#if the first row of the file can act as a header or not.\n",
    "#if header=None then usecols is used to give column names.\n",
    "#nrows is used to pick no of rows from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df=pd.read_csv('random_tweets.csv',sep=',',header=None,usecols=range(0,10),nrows=12000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Removal of links, @, and hashtags and emojis from the tweets.\n",
    "2. Corecting the encoding of the broken code using ftfy.\n",
    "3. Expanding contracted text.\n",
    "4. Removing of punctuations.\n",
    "5. Removal of stopwords.\n",
    "6. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all contractions are taken from the given link https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clist= {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_re = re.compile('(%s)' % '|'.join(clist.keys()))\n",
    "#'|'.join(clist.keys()) is used to join all list members returned by clist.keys() \n",
    "#%s is string formatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandContraction(text,c_re=c_re):\n",
    "    def replace(match):\n",
    "        return clist[match.group(0)]\n",
    "    return c_re.sub(replace,text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweets(tweets):\n",
    "    c_t=[]      #array that will hold all tweets after cleaning and will be returned\n",
    "    #working on each tweet.\n",
    "    for tweet in tweets:\n",
    "        tweet=str(tweet)\n",
    "        #if the tweets doesnt contain URLs\n",
    "        if re.match(\"(\\w+:\\/\\/\\S+)\", tweet) == None and len(tweet) > 10:\n",
    "            #strings starting with https://\n",
    "            #match function return a match object if the pattern is there in the stirng otherwise return None\n",
    "            tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(\\#[A-Za-z0-9]+)|(<Emoji:.*>)|(pic\\.twitter\\.com\\/.*)\", \" \", tweet).split())\n",
    "            #re.sub function is used to replace all occurences of a pattern in the given string\n",
    "            #the property of hastags and tags that they are continuous after a @ or # sign is used to make RE.\n",
    "        tweet=ftfy.fix_text(tweet)#fixing faulty encoded text\n",
    "        tweet = expandContraction(tweet)\n",
    "        tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", tweet).split())#removing puctuations\n",
    "        #removing stop words\n",
    "        s_w=set(stopwords.words('english'))\n",
    "        w_t=nltk.tokenize.word_tokenize(tweet)#creates a list of all words in tweet\n",
    "        fil_sen=[w for w in w_t if not w in s_w]\n",
    "        tweet=' '.join(fil_sen)\n",
    "        #stemming\n",
    "        tweet=PorterStemmer().stem(tweet)\n",
    "        c_t.append(tweet)\n",
    "        \n",
    "    return c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_arr=[x for x in depression_df[6]]\n",
    "ran_arr=[x for x in random_df[6]]\n",
    "fin_dep=cleanTweets(dep_arr)\n",
    "fin_ran=cleanTweets(ran_arr)\n",
    "no_of_replies_dep=[x for x in depression_df[7]]\n",
    "no_of_replies_ran=[x for x in random_df[7]]\n",
    "\n",
    "len(no_of_replies_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(no_of_replies_dep)):\n",
    "    if(no_of_replies_dep[i].isnumeric()==False):\n",
    "        no_of_replies_dep[i]='0'\n",
    "        \n",
    "for i in range(0,len(no_of_replies_ran)):\n",
    "    if(isinstance(no_of_replies_ran[i],float)):\n",
    "        no_of_replies_ran[i]='0'\n",
    "    if(no_of_replies_ran[i].isnumeric()==False):\n",
    "        no_of_replies_ran[i]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0,len(no_of_replies_dep)):\n",
    "    no_of_replies_dep[i]=int(no_of_replies_dep[i])\n",
    "    \n",
    "for i in range(0,len(no_of_replies_ran)):\n",
    "    no_of_replies_ran[i]=int(no_of_replies_ran[i])\n",
    "    \n",
    "len(no_of_replies_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_of_replies_dep[1]\n",
    "\n",
    "#Normalise the numpy array\n",
    "max1 = max(no_of_replies_dep)\n",
    "no_of_replies_dep = [float(i)/max1 for i in no_of_replies_dep]\n",
    "\n",
    "\n",
    "max2 = max(no_of_replies_ran)\n",
    "no_of_replies_ran = [float(i)/max2 for i in no_of_replies_ran]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#x = np.random.rand(1000)*10\n",
    "#n1 = x/np.linalg.norm(x)\n",
    "#no_of_replies_dep = normalize(x[:, np.newaxis], axis = 0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03225806451612903"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_replies_dep[2940]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_replies_ran[2940]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing is used to convert text into tokens. Each word is given a unique integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(20000)\n",
    "tokenizer.fit_on_texts(fin_dep+fin_ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#prints dictionary of words and their integer values using tokenizer.word_index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now sequence of words are being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dep=tokenizer.texts_to_sequences(fin_dep)\n",
    "seq_ran=tokenizer.texts_to_sequences(fin_ran)\n",
    "#text_to_word_sequence is used to create sequence of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#a list of sublists is created. Each tweet is a sublist. instead of words thier integer values are taken....(seq_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(tokenizer.word_index))\n",
    "#no of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d=pad_sequences(seq_dep,maxlen=140)\n",
    "data_r=pad_sequences(seq_ran,maxlen=140)\n",
    "#pad_sequence is used to make every sequence of same length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 140)\n"
     ]
    }
   ],
   "source": [
    "print(data_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word Embedding is a language modeling technique used for mapping words to vectors of real numbers. It represents words or phrases in vector space with several dimensions. Word embeddings can be generated using various methods like neural networks, co-occurrence matrix, kmin models, etc. \n",
    "\n",
    "Word2Vec consists of models for generating word embedding. These models are shallow two layer neural networks having one input layer, one hidden layer and one output layer. Word2Vec utilizes two architectures :\n",
    "\n",
    "    CBOW (Continuous Bag of Words) : CBOW model predicts the current word given context words within specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent current word present at the output layer\n",
    "    \n",
    "    Skip Gram : Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Link of pretrained model is  from here https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vec=KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\",binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding matrix is a n x m matrix where n is the number of words and m is the dimension of the embedding. In this case, m=300 and n=20000. We take the min between the number of unique words in our tokenizer and max words in case there are less unique words than the max we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_words=min(20000,len(tokenizer.word_index))\n",
    "\n",
    "emb_matrix=np.zeros((min_words,300))\n",
    "\n",
    "#In this case since unique words is greater than 20000 np.zeros returns a 2d array of specified size but filled with zeros\n",
    "for (word,idx) in tokenizer.word_index.items(): #.items() fn returns a dictionary\n",
    "    if word in word2vec.vocab and idx<20000: #while tokenizing each word was given an integer value which acts as an index in emb_matrix\n",
    "        emb_matrix[idx]=word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting and formatting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assigning labels to the depressive tweets and random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPRES_NROWS = 3200  # number of rows to read from DEPRESSIVE_TWEETS_CSV\n",
    "RANDOM_NROWS = 12000 # number of rows to read from RANDOM_TWEETS_CSV\n",
    "MAX_SEQUENCE_LENGTH = 140 # Max tweet size\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "TRAIN_SPLIT = 0.6\n",
    "TEST_SPLIT = 0.2\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_d=np.array([1]*3200) #the argument will create an array of 3200 1s since there are 3200 depressive tweets \n",
    "labels_r=np.array([0]*12000) #the argument will create an array of 12000 0s since there are 12000 random tweets\n",
    "replies_dep_np=np.array(no_of_replies_dep)\n",
    "replies_ran_np=np.array(no_of_replies_ran)\n",
    "# so depressive tweets are labeled as 1 and random tweets as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_d = np.random.permutation(len(data_d))\n",
    "idx_train_d = perm_d[:int(len(data_d)*(TRAIN_SPLIT))]\n",
    "idx_test_d = perm_d[int(len(data_d)*(TRAIN_SPLIT)):int(len(data_d)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
    "idx_val_d = perm_d[int(len(data_d)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
    "\n",
    "perm_r = np.random.permutation(len(data_r))\n",
    "idx_train_r = perm_r[:int(len(data_r)*(TRAIN_SPLIT))]\n",
    "idx_test_r = perm_r[int(len(data_r)*(TRAIN_SPLIT)):int(len(data_r)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
    "idx_val_r = perm_r[int(len(data_r)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
    "\n",
    "# Combine depressive tweets and random tweets arrays\n",
    "data_train = np.concatenate((data_d[idx_train_d], data_r[idx_train_r]))\n",
    "labels_train = np.concatenate((labels_d[idx_train_d], labels_r[idx_train_r]))\n",
    "replies_train=np.concatenate((replies_dep_np[idx_train_d],replies_ran_np[idx_train_r]))\n",
    "data_test = np.concatenate((data_d[idx_test_d], data_r[idx_test_r]))\n",
    "labels_test = np.concatenate((labels_d[idx_test_d], labels_r[idx_test_r]))\n",
    "replies_test=np.concatenate((replies_dep_np[idx_test_d],replies_ran_np[idx_test_r]))\n",
    "data_val = np.concatenate((data_d[idx_val_d], data_r[idx_val_r]))\n",
    "labels_val = np.concatenate((labels_d[idx_val_d], labels_r[idx_val_r]))\n",
    "replies_val=np.concatenate((replies_dep_np[idx_val_d],replies_ran_np[idx_val_r]))\n",
    "\n",
    "# Shuffling\n",
    "perm_train = np.random.permutation(len(data_train))\n",
    "data_train = data_train[perm_train]\n",
    "labels_train = labels_train[perm_train]\n",
    "replies_train=replies_train[perm_train]\n",
    "perm_test = np.random.permutation(len(data_test))\n",
    "data_test = data_test[perm_test]\n",
    "labels_test = labels_test[perm_test]\n",
    "replies_test=replies_test[perm_test]\n",
    "perm_val = np.random.permutation(len(data_val))\n",
    "data_val = data_val[perm_val]\n",
    "labels_val =labels_val[perm_val]\n",
    "replies_val=replies_val[perm_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_input=Input(shape=(140,))\n",
    "embd=Embedding(len(emb_matrix), EMBEDDING_DIM, weights=[emb_matrix],input_length=MAX_SEQUENCE_LENGTH, trainable=False)(main_input)\n",
    "\n",
    "x=Input(shape=(1,))\n",
    "conv=Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(embd)\n",
    "maxpool=MaxPooling1D(pool_size=2)(conv)\n",
    "drop1=Dropout(0.2)(maxpool)\n",
    "lstm=LSTM(300,dropout=0.2)(drop1)\n",
    "\n",
    "#concatenate main and aux inputs lstm and x\n",
    "conc=concatenate([lstm,x])\n",
    "drop2=Dropout(0.2)(conc)\n",
    "dense=Dense(1,activation='sigmoid')(drop2)\n",
    "\n",
    "#Model prep\n",
    "model=Model([main_input,x],dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 140)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 140, 300)     6000000     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 140, 32)      28832       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 70, 32)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 70, 32)       0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 300)          399600      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 301)          0           lstm_4[0][0]                     \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 301)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            302         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,428,734\n",
      "Trainable params: 428,734\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit([headline_data, additional_data], [labels, labels], epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9120 samples, validate on 3040 samples\n",
      "Epoch 1/10\n",
      "9120/9120 [==============================] - 39s 4ms/step - loss: 0.0196 - acc: 0.9941 - val_loss: 0.0703 - val_acc: 0.9878\n",
      "Epoch 2/10\n",
      "9120/9120 [==============================] - 40s 4ms/step - loss: 0.0172 - acc: 0.9951 - val_loss: 0.0739 - val_acc: 0.9878\n",
      "Epoch 3/10\n",
      "9120/9120 [==============================] - 39s 4ms/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0744 - val_acc: 0.9865\n",
      "Epoch 4/10\n",
      "9120/9120 [==============================] - 39s 4ms/step - loss: 0.0122 - acc: 0.9964 - val_loss: 0.0798 - val_acc: 0.9862\n",
      "Epoch 5/10\n",
      "9120/9120 [==============================] - 40s 4ms/step - loss: 0.0287 - acc: 0.9913 - val_loss: 0.0818 - val_acc: 0.9862\n",
      "Epoch 6/10\n",
      "9120/9120 [==============================] - 39s 4ms/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.0780 - val_acc: 0.9859\n",
      "Epoch 7/10\n",
      "9120/9120 [==============================] - 38s 4ms/step - loss: 0.0114 - acc: 0.9969 - val_loss: 0.0993 - val_acc: 0.9868\n",
      "Epoch 8/10\n",
      "9120/9120 [==============================] - 38s 4ms/step - loss: 0.0099 - acc: 0.9974 - val_loss: 0.0890 - val_acc: 0.9878\n",
      "Epoch 9/10\n",
      "9120/9120 [==============================] - 38s 4ms/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.0833 - val_acc: 0.9859\n",
      "Epoch 10/10\n",
      "9120/9120 [==============================] - 38s 4ms/step - loss: 0.0102 - acc: 0.9965 - val_loss: 0.0964 - val_acc: 0.9859\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "hist = model.fit([data_train,replies_train], labels_train, \\\n",
    "        validation_data=([data_val,replies_val],labels_val), \\\n",
    "        epochs=EPOCHS, batch_size=40, shuffle=True, \\\n",
    "        callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.49%\n"
     ]
    }
   ],
   "source": [
    "labels_pred = model.predict([data_test,replies_test])\n",
    "labels_pred = np.round(labels_pred.flatten())\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "# Embedded layer\n",
    "model.add(Embedding(len(emb_matrix), EMBEDDING_DIM, weights=[emb_matrix],input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "# Convolutional Layer\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "# LSTM Layer\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9120 samples, validate on 3040 samples\n",
      "Epoch 1/10\n",
      "9120/9120 [==============================] - 92s 10ms/step - loss: 0.1558 - acc: 0.9492 - val_loss: 0.0908 - val_acc: 0.9806\n",
      "Epoch 2/10\n",
      "9120/9120 [==============================] - 36s 4ms/step - loss: 0.0696 - acc: 0.9840 - val_loss: 0.0681 - val_acc: 0.9826\n",
      "Epoch 3/10\n",
      "9120/9120 [==============================] - 36s 4ms/step - loss: 0.0600 - acc: 0.9846 - val_loss: 0.0628 - val_acc: 0.9829\n",
      "Epoch 4/10\n",
      "9120/9120 [==============================] - 37s 4ms/step - loss: 0.0903 - acc: 0.9723 - val_loss: 0.0699 - val_acc: 0.9816\n",
      "Epoch 5/10\n",
      "9120/9120 [==============================] - 35s 4ms/step - loss: 0.0506 - acc: 0.9870 - val_loss: 0.0640 - val_acc: 0.9839\n",
      "Epoch 6/10\n",
      "9120/9120 [==============================] - 36s 4ms/step - loss: 0.0360 - acc: 0.9909 - val_loss: 0.0765 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      "9120/9120 [==============================] - 36s 4ms/step - loss: 0.0314 - acc: 0.9914 - val_loss: 0.0690 - val_acc: 0.9849\n",
      "Epoch 8/10\n",
      "9120/9120 [==============================] - 36s 4ms/step - loss: 0.0270 - acc: 0.9927 - val_loss: 0.0696 - val_acc: 0.9845\n",
      "Epoch 9/10\n",
      "9120/9120 [==============================] - 36s 4ms/step - loss: 0.0201 - acc: 0.9948 - val_loss: 0.0782 - val_acc: 0.9839\n",
      "Epoch 10/10\n",
      "9120/9120 [==============================] - 36s 4ms/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0746 - val_acc: 0.9836\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "hist = model.fit(data_train, labels_train, \\\n",
    "        validation_data=(data_val, labels_val), \\\n",
    "        epochs=EPOCHS, batch_size=40, shuffle=True, \\\n",
    "        callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras, we can implement early stopping as a callback function. Callbacks are functions that can be applied at certain stages of the training process, such as at the end of each epoch. Specifically, in our solution, we included EarlyStopping(monitor='val_loss', patience=2) to define that we wanted to monitor the test (validation) loss at each epoch and after the test loss has not improved after two epochs, training is interrupted. However, since we set patience=2, we won’t get the best model, but the model two epochs after the best model. Therefore, optionally, we can include a second operation, ModelCheckpoint which saves the model to a file after every checkpoint (which can be useful in case a multi-day training session is interrupted for some reason. Helpful for us, if we set save_best_only=True then ModelCheckpoint will only save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.52%\n"
     ]
    }
   ],
   "source": [
    "labels_pred = model.predict([data_test,replies_test])\n",
    "labels_pred = np.round(labels_pred.flatten())\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2400\n",
      "           1       0.97      0.96      0.96       640\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      3040\n",
      "   macro avg       0.98      0.98      0.98      3040\n",
      "weighted avg       0.99      0.99      0.99      3040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_test, labels_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
